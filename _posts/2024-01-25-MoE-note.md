# MoE 笔记

## Illustration


## Mixtral MoE



## DeepSeek MoE

看了下设置还是有点非主流，跟Mistral-MOE差别比较大 
1. 激活参数量跟总参数量差别比较大 2.8B/16.4B，一般4倍左右
2. 激活experts比例比较小 6/64，一般是1/4左右
3. 没有做top k expert的weights normalization，一般都会做，让权重和为1
4. 常规experts之外，存在2个shared experts，所有token必然会选择
5. 第一个layer的MLP是正常MLP，其余是MOE MLP
6. 还有expert的intermediate size非常小，比正常的MLP intermediate size小一个数量级